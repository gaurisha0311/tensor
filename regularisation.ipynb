{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956ce1d8-ab93-4423-91d5-3f14c31b2f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is a set of techniques used in deep learning to prevent overfitting by adding a penalty term to the \\nloss \\nfunction. It discourages overly complex models and encourages smoother models that generalize well to unseen data.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "#1\n",
    "'''\n",
    "Regularization is a set of techniques used in deep learning to prevent overfitting by adding a penalty term to the \n",
    "loss \n",
    "function. It discourages overly complex models and encourages smoother models that generalize well to unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64ae3f6-4a9d-4e29-b6c9-496198520ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nt is the balance between underfitting (high bias) and overfitting (high variance). A model with high complexity\\nmay fit the training data well (low bias) but generalize poorly (high variance).\\nRegularization's Role: Regularization helps strike a balance in the bias-variance tradeoff by penalizing overly\\ncomplex models. It reduces variance by discouraging extreme weights, leading to a model that generalizes better.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "'''\n",
    "t is the balance between underfitting (high bias) and overfitting (high variance). A model with high complexity\n",
    "may fit the training data well (low bias) but generalize poorly (high variance).\n",
    "Regularization's Role: Regularization helps strike a balance in the bias-variance tradeoff by penalizing overly\n",
    "complex models. It reduces variance by discouraging extreme weights, leading to a model that generalizes better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a645f25-b028-479f-a729-e529c52cfefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization helps \\nprevent overfitting by adding a penalty for complex models.\\nIt discourages the learning of noise in the training data.\\nRegularization techniques, such as dropout and early stopping, are commonly used to improve the generalization of\\ndeep learning models.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "'''\n",
    "Regularization helps \n",
    "prevent overfitting by adding a penalty for complex models.\n",
    "It discourages the learning of noise in the training data.\n",
    "Regularization techniques, such as dropout and early stopping, are commonly used to improve the generalization of\n",
    "deep learning models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae85e58-96c4-47ad-bdc3-5ce37780c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplanation: Dropout \\nrandomly deactivates a fraction of neurons during training, preventing the reliance on specific neurons and\\npromoting a more robust network.\\nImpact: Reduces overfitting, improves generalization, and acts as an ensemble method by training different \\nsubnetworks.\\nTraining and Inference: During training, neurons are randomly dropped; during inference, all neurons are used but\\nwith reduced weights.Explanation: Dropout randomly deactivates a fraction of neurons during training, preventing \\nthe reliance on specific neurons and promoting a more robust network.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "#5\n",
    "'''\n",
    "Explanation: Dropout \n",
    "randomly deactivates a fraction of neurons during training, preventing the reliance on specific neurons and\n",
    "promoting a more robust network.\n",
    "Impact: Reduces overfitting, improves generalization, and acts as an ensemble method by training different \n",
    "subnetworks.\n",
    "Training and Inference: During training, neurons are randomly dropped; during inference, all neurons are used but\n",
    "with reduced weights.Explanation: Dropout randomly deactivates a fraction of neurons during training, preventing \n",
    "the reliance on specific neurons and promoting a more robust network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba7abe5-6293-4704-ac56-c8aae00385cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noncept: Early \\nstopping involves monitoring the model's performance on a validation set and stopping training when the performance starts to degrade.\\nPreventing Overfitting: Stops the training process before the model overfits the training data, ensuring better\\ngeneralization.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "'''\n",
    "oncept: Early \n",
    "stopping involves monitoring the model's performance on a validation set and stopping training when the performance starts to degrade.\n",
    "Preventing Overfitting: Stops the training process before the model overfits the training data, ensuring better\n",
    "generalization.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8712291-2434-4fa4-9c2b-1b59d8e44598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplanation: Batch Normalization normalizes\\nthe inputs of a layer to have zero mean and unit variance. It includes learnable parameters to scale and shift the \\nnormalized outputs.\\nRole as Regularization: Acts as a form of regularization by reducing internal covariate shift, making the training\\nmore stable and improving generalization.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "'''\n",
    "Explanation: Batch Normalization normalizes\n",
    "the inputs of a layer to have zero mean and unit variance. It includes learnable parameters to scale and shift the \n",
    "normalized outputs.\n",
    "Role as Regularization: Acts as a form of regularization by reducing internal covariate shift, making the training\n",
    "more stable and improving generalization.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88f7fec-9c42-48e3-a56f-db405e36056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 12:07:27.425895: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-14 12:07:27.492304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-14 12:07:27.492365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-14 12:07:27.493829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-14 12:07:27.502741: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-14 12:07:27.503812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-14 12:07:28.701168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 1s 9ms/step - loss: 2.0530 - accuracy: 0.3377 - val_loss: 1.6609 - val_accuracy: 0.5278\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.2516 - accuracy: 0.7267 - val_loss: 0.9589 - val_accuracy: 0.7986\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6627 - accuracy: 0.8695 - val_loss: 0.5456 - val_accuracy: 0.8681\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3737 - accuracy: 0.9269 - val_loss: 0.3802 - val_accuracy: 0.8993\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.9513 - val_loss: 0.3130 - val_accuracy: 0.9028\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9652 - val_loss: 0.2678 - val_accuracy: 0.9236\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9791 - val_loss: 0.2444 - val_accuracy: 0.9236\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1059 - accuracy: 0.9826 - val_loss: 0.2252 - val_accuracy: 0.9271\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9878 - val_loss: 0.2094 - val_accuracy: 0.9306\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9896 - val_loss: 0.2003 - val_accuracy: 0.9306\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 1s 8ms/step - loss: 2.5399 - accuracy: 0.1027 - val_loss: 2.0482 - val_accuracy: 0.3750\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.1328 - accuracy: 0.2437 - val_loss: 1.8200 - val_accuracy: 0.5069\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.9446 - accuracy: 0.3185 - val_loss: 1.6043 - val_accuracy: 0.6319\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.7827 - accuracy: 0.3847 - val_loss: 1.3740 - val_accuracy: 0.7153\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.6063 - accuracy: 0.4369 - val_loss: 1.1739 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.4527 - accuracy: 0.4952 - val_loss: 0.9874 - val_accuracy: 0.8090\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.3226 - accuracy: 0.5413 - val_loss: 0.8394 - val_accuracy: 0.8264\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.1869 - accuracy: 0.5918 - val_loss: 0.7052 - val_accuracy: 0.8715\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0985 - accuracy: 0.6214 - val_loss: 0.6072 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0721 - accuracy: 0.6327 - val_loss: 0.5529 - val_accuracy: 0.8854\n",
      "Model without Dropout:\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9556\n",
      "\n",
      "Model with Dropout:\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.510745644569397, 0.8999999761581421]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the dataset (as an example, using sklearn's load_digits)\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple neural network model without Dropout\n",
    "model_without_dropout = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Assuming a classification task with 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_without_dropout.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model without Dropout\n",
    "model_without_dropout.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Build a simple neural network model with Dropout\n",
    "model_with_dropout = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.5),  # Adding Dropout layer with a dropout rate of 0.5\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_dropout.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Dropout\n",
    "model_with_dropout.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Model without Dropout:\")\n",
    "model_without_dropout.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nModel with Dropout:\")\n",
    "model_with_dropout.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee4cb0-9e86-4607-bee9-4339d440a493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
