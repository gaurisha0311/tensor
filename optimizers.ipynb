{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b12161b-d2d2-4b2a-8904-390a7eb93f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOptimization algorithms play a crucial role in training artificial neural networks by minimizing a chosen loss \\nfunction.\\nThe goal is to find the optimal set of parameters (weights and biases) that result in the best model performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "#1\n",
    "'''\n",
    "Optimization algorithms play a crucial role in training artificial neural networks by minimizing a chosen loss \n",
    "function.\n",
    "The goal is to find the optimal set of parameters (weights and biases) that result in the best model performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560d17b5-844a-4cb5-ad90-b5b151802c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient Descent (GD): GD is an iterative optimization algorithm that minimizes the loss function by adjusting the\\nmodel parameters in the opposite direction of the gradient. It involves calculating the gradient of the loss with \\nrespect to each parameter and updating the parameters in the negative gradient direction.\\n\\nVariants of GD:\\n\\nStochastic Gradient Descent (SGD): In SGD, only a subset (a mini-batch) of training data is used to compute the \\ngradient and update the parameters. It introduces stochasticity and can speed up convergence.\\nMini-batch Gradient Descent: A compromise between GD and SGD, mini-batch GD updates parameters using a small batch\\nof data, balancing the benefits of both GD and SGD.\\nBatch Gradient Descent: The original GD, which updates parameters using the entire training dataset.\\nDifferences and Tradeoffs:\\n\\nConvergence Speed: SGD and its variants often converge faster due to more frequent updates. However, mini-batch \\nsizes can affect the tradeoff between speed and accuracy.\\nMemory Requirements: Batch GD requires more memory as it processes the entire dataset at once, whereas SGD and \\nmini-batch GD are more memory-efficient.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "'''\n",
    "Gradient Descent (GD): GD is an iterative optimization algorithm that minimizes the loss function by adjusting the\n",
    "model parameters in the opposite direction of the gradient. It involves calculating the gradient of the loss with \n",
    "respect to each parameter and updating the parameters in the negative gradient direction.\n",
    "\n",
    "Variants of GD:\n",
    "\n",
    "Stochastic Gradient Descent (SGD): In SGD, only a subset (a mini-batch) of training data is used to compute the \n",
    "gradient and update the parameters. It introduces stochasticity and can speed up convergence.\n",
    "Mini-batch Gradient Descent: A compromise between GD and SGD, mini-batch GD updates parameters using a small batch\n",
    "of data, balancing the benefits of both GD and SGD.\n",
    "Batch Gradient Descent: The original GD, which updates parameters using the entire training dataset.\n",
    "Differences and Tradeoffs:\n",
    "\n",
    "Convergence Speed: SGD and its variants often converge faster due to more frequent updates. However, mini-batch \n",
    "sizes can affect the tradeoff between speed and accuracy.\n",
    "Memory Requirements: Batch GD requires more memory as it processes the entire dataset at once, whereas SGD and \n",
    "mini-batch GD are more memory-efficient.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43a21bc-5c8b-4be3-af30-559cf626095a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStochastic Gradient Descent (SGD):\\n\\nAdvantages: Faster convergence, reduced memory requirements, and the ability to escape from local minima.\\nLimitations: Stochastic nature may introduce noise; convergence can be oscillatory.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "#5\n",
    "'''\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Advantages: Faster convergence, reduced memory requirements, and the ability to escape from local minima.\n",
    "Limitations: Stochastic nature may introduce noise; convergence can be oscillatory.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33230ec-b3a5-4f97-ab42-7217a2b84c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdam combines momentum and adaptive learning rates.\\nBenefits: Efficient in terms of both convergence and memory, adapts learning rates for each parameter individually.\\nDrawbacks: Sensitive to hyperparameter choices, may exhibit erratic behavior in some cases.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "'''\n",
    "Adam combines momentum and adaptive learning rates.\n",
    "Benefits: Efficient in terms of both convergence and memory, adapts learning rates for each parameter individually.\n",
    "Drawbacks: Sensitive to hyperparameter choices, may exhibit erratic behavior in some cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e39976-98cb-44f1-b7b3-c9c75fb3c7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRMSprop Optimizer:\\n\\nConcept: RMSprop adjusts the learning rates based on the moving average of the squared gradients.\\nStrengths: Addresses the challenges of adaptive learning rates, effective in non-stationary environments.\\nWeaknesses: May still require careful tuning of hyperparameters.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "'''\n",
    "RMSprop Optimizer:\n",
    "\n",
    "Concept: RMSprop adjusts the learning rates based on the moving average of the squared gradients.\n",
    "Strengths: Addresses the challenges of adaptive learning rates, effective in non-stationary environments.\n",
    "Weaknesses: May still require careful tuning of hyperparameters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503770c3-302e-433c-a09f-e085761bec1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#q3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#q3\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X_train, y_train are your training features and labels\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Stochastic Gradient Descent (SGD) optimizer\n",
    "sgd_optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "sgd_history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "adam_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "adam_history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "rmsprop_optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=rmsprop_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rmsprop_history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Compare the impact on model convergence and performance using the history objects\n",
    "# (e.g., sgd_history, adam_history, rmsprop_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b89b36a-e449-4f8b-a02f-dda71ace358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.5/186.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.26.2 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.35.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba8d110-d11e-416a-8fa2-035a870f9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 11:57:32.107631: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-14 11:57:32.172793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-14 11:57:32.172861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-14 11:57:32.174406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-14 11:57:32.183530: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-14 11:57:32.184723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-14 11:57:33.434581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assume X_train, y_train are your training features and labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 10\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b14780-0634-4224-b7bf-91c224120e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 1s 8ms/step - loss: 2.2610 - accuracy: 0.1636 - val_loss: 2.1060 - val_accuracy: 0.3021\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.9291 - accuracy: 0.3716 - val_loss: 1.8231 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.6640 - accuracy: 0.5622 - val_loss: 1.5860 - val_accuracy: 0.6354\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.4291 - accuracy: 0.6832 - val_loss: 1.3792 - val_accuracy: 0.7396\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.2229 - accuracy: 0.7441 - val_loss: 1.2015 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0472 - accuracy: 0.7876 - val_loss: 1.0532 - val_accuracy: 0.8021\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.9005 - accuracy: 0.8181 - val_loss: 0.9334 - val_accuracy: 0.8368\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7801 - accuracy: 0.8486 - val_loss: 0.8371 - val_accuracy: 0.8542\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.8747 - val_loss: 0.7591 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.8886 - val_loss: 0.6955 - val_accuracy: 0.8681\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 1s 7ms/step - loss: 0.4358 - accuracy: 0.9199 - val_loss: 0.4582 - val_accuracy: 0.9097\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.9556 - val_loss: 0.3478 - val_accuracy: 0.9306\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.9730 - val_loss: 0.2868 - val_accuracy: 0.9410\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1139 - accuracy: 0.9800 - val_loss: 0.2590 - val_accuracy: 0.9410\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9852 - val_loss: 0.2327 - val_accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9904 - val_loss: 0.2197 - val_accuracy: 0.9444\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9948 - val_loss: 0.2057 - val_accuracy: 0.9444\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9974 - val_loss: 0.2037 - val_accuracy: 0.9479\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.9974 - val_loss: 0.1917 - val_accuracy: 0.9479\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.9991 - val_loss: 0.1851 - val_accuracy: 0.9514\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 1s 7ms/step - loss: 0.0239 - accuracy: 0.9983 - val_loss: 0.1765 - val_accuracy: 0.9514\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9991 - val_loss: 0.1757 - val_accuracy: 0.9514\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9991 - val_loss: 0.1655 - val_accuracy: 0.9514\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1653 - val_accuracy: 0.9549\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1609 - val_accuracy: 0.9583\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1759 - val_accuracy: 0.9549\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1688 - val_accuracy: 0.9549\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1612 - val_accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9653\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the dataset (as an example, using sklearn's load_digits)\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Assuming a classification task with 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model with Stochastic Gradient Descent (SGD) optimizer\n",
    "sgd_optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "sgd_history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "adam_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "adam_history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "rmsprop_optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=rmsprop_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rmsprop_history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Compare the impact on model convergence and performance using the history objects\n",
    "# (e.g., sgd_history, adam_history, rmsprop_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacf917-22f8-4869-b180-0d43f99d8f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
